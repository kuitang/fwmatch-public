classdef SingleLoopyModel
    %SINGLELOOPYMODEL On single model (generated by one combination of
    %regularization parameters and density).
    
    properties
        % training and test sets
        x_train, x_test;
        
        % struct with 
        %       F (2xnode_count matrix with node weights) 
        %       G (4xedge_count matrix with edge weights), 
        %       node_potentials (column vector),
        %       edge_potentials (symmetric matrix),
        %       true_logZ (computed with JTA)
        %       logZ (computed with Bethe approx.)
        theta;
        
        %   s_lambda: regularization of structure learning
        %   p_lambda: regularization of parameter learning
        %   density: target density of the structure
        s_lambda, p_lambda, density;
        
        %   structure: NxN binary adjacency matrix
        %   variable names: cell array with names for each variable
        structure, variable_names;
        
        %   max_degree: maximum number of connection to a node
        max_degree, median_degree, mean_degree, rms_degree;
        reweight;
        
        %   train_likelihood: avg. (per sample) likelihood of training set
        %   test_likelihood: avg. (per sample) likelihood of test set
        %   true_test_likelihood: avg. (per sample) likelihood of test set
        train_likelihood, true_train_likelihood; 
        test_likelihood, true_test_likelihood;
        
        %   true_node_marginals: marginals stored when we run JTA to get
        %       true_logZ. These marginals are used to mix the loopy model
        %       with the temporal models
        true_node_marginals;
        
        % temporal model parameters
        temporal_model, default_t_lambda, is_temporal_model_trained;
        
        % cell array with temporal model learned parameters, may change
        % format depending on temporal model
        temporal_conditionals;
        
        % Inference variables
        if_true_conditional_likelihood, max_prediction_iter, approx_predict_opts;
    end
    
    methods
        
        % Constructor: used by LoopyModelCollection objects
        function self = SingleLoopyModel(x_train, x_test, model_struct, variable_names)
            self.x_train = x_train;
            self.x_test = x_test;
            self.variable_names = variable_names;
            
            self.theta = model_struct.theta;
            self.s_lambda = model_struct.s_lambda;
            self.p_lambda = model_struct.p_lambda;
            self.density = model_struct.density;
            self.structure = model_struct.structure;
%             self.train_likelihood = model_struct.train_likelihood;
%             self.test_likelihood = model_struct.test_likelihood;
            self.max_degree = model_struct.max_degree;
            if isfield(model_struct, 'true_test_likelihood')
                self.true_test_likelihood = model_struct.true_test_likelihood;
                self.true_node_marginals = model_struct.true_node_marginals;
            end
%             if isfield(model_struct, 'true_train_likelihood')
%                 self.true_train_likelihood = model_struct.true_train_likelihood;
%             end
            if isfield(model_struct, 'median_degree')
                self.median_degree = model_struct.median_degree;
            end
            if isfield(model_struct, 'mean_degree')
                self.mean_degree = model_struct.mean_degree;
            end
            if isfield(model_struct, 'rms_degree')
                self.rms_degree = model_struct.rms_degree;
            end
            if isfield(model_struct, 'reweight')
                self.reweight = model_struct.reweight;
            end
            
            self.is_temporal_model_trained = false;
        end
        
        function plot_graph(self)
            plot_graph(self.theta.edge_potentials, ...
                'node_names', self.variable_names, ...
                'color_edges', true);
        end
        
        %% Inference
        % Trains inference model
        function self = inference_model(self, varargin)
            
            parser = inputParser;
            
            parser.addOptional('if_true_conditional_likelihood',false, @(x) islogical(x));
            parser.addOptional('max_prediction_iter', 5000, @(x) (x - floor(x) == 0) && (x > 0));
            
            % approximate inference options
            parser.addOptional('reweight', 0.5, @isscalar);
            parser.addOptional('TolGap', 1e-10);
            parser.addOptional('errNegDualityGap', false);
            parser.addOptional('lineSearchOpts', optimset('TolX', 1e-7));
            parser.addOptional('checkStuck', false);
            
            parser.parse(varargin{:});
            
            self.if_true_conditional_likelihood = parser.Results.if_true_conditional_likelihood;
            self.max_prediction_iter = parser.Results.max_prediction_iter;
            self.approx_predict_opts = {'reweight',parser.Results.reweight,'TolGap',parser.Results.TolGap,...
                'errNegDualityGap',parser.Results.errNegDualityGap,'lineSearchOpts',parser.Results.lineSearchOpts,...
                'checkStuck',parser.Results.checkStuck};
        end
        
        function [conditional_likelihood] = compute_conditional_likelihood(self,observed_variables,dataset_type)
            if strcmp(dataset_type,'train')
                dataset = self.x_train;
            else
                dataset = self.x_test;
            end
            if self.if_true_conditional_likelihood
                conditional_likelihood.true = 0;
            end
            conditional_likelihood.approx = 0;
            for i = 1:size(dataset,1)
                if self.if_true_conditional_likelihood
                    conditional_likelihood.true = conditional_likelihood.true + ...
                        self.compute_conditional_likelihood_for_sample_true(dataset(i,:),observed_variables);
                end
                conditional_likelihood.approx = conditional_likelihood.approx + ...
                    self.compute_conditional_likelihood_for_sample_approx(dataset(i,:),observed_variables);
            end
            conditional_likelihood.approx = conditional_likelihood.approx / size(dataset,1);
            if self.if_true_conditional_likelihood
                conditional_likelihood.true = conditional_likelihood.true / size(dataset,1);
            end
        end
        
        % This function implements slicing to get exact inference
        function [sample_conditional_likelihood] = compute_conditional_likelihood_for_sample_true(self,sample,observed_variables)
            % modify parameters, i.e. overcomplete node potentials 
            F = self.theta.F;
            G = self.theta.G;
            
            % For all nodes, where sample(node) == 1, put F(1,node) = small
            % Find all such nodes in the sample
            nodes_1 = intersect(find(sample == 1),observed_variables);
            F_indices = 1*ones(1,length(nodes_1));
            F(sub2ind(size(F),F_indices,nodes_1)) = -1000;
            
            % For all nodes, where sample(node) == 0, put F(2,node) = small
            % Find all such nodes in the sample
            nodes_0 = intersect(find(sample == 0),observed_variables);
            F_indices = 2*ones(1,length(nodes_0));
            F(sub2ind(size(F),F_indices,nodes_0)) = -1000;
            
            % Get overcomplete parametrization for the sample
            overcomplete_struct = samples_to_overcomplete(sample, self.structure);
            edge_list = overcomplete_struct.edges{1}';
            
            % Remove all the edges where both nodes are observed
            [~,ind] = ismember(edge_list(:,1),observed_variables);
            observed_edges = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),observed_variables);
            observed_edges = intersect(observed_edges,find(ind ~= 0));
            edge_list(observed_edges,:) = [];
            G(:,observed_edges) = [];
            
            % Recompute overcomplete struct
            % change structure
            modified_structure = self.structure;
            x = repmat(observed_variables',length(observed_variables),1);
            
            y = repmat(observed_variables',1,length(observed_variables))';
            y = y(:);
            
            modified_structure(sub2ind(size(modified_structure),x,y)) = 0;
            
            % For all edges, where sample(node_i) = 0 and node_i observed 
            % variable, 
            % put G(1*,(node_i,*)) = -1000 and G(*1,(*,node_i)) = -1000
            [~,ind] = ismember(edge_list(:,1),nodes_1);
            edges_00_01 = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),nodes_1);
            edges_00_10 = find(ind ~= 0);
            
            G(sub2ind(size(G),ones(size(edges_00_01)),edges_00_01)) = -1000;
            G(sub2ind(size(G),3*ones(size(edges_00_01)),edges_00_01)) = -1000;
            G(sub2ind(size(G),ones(size(edges_00_10)),edges_00_10)) = -1000;
            G(sub2ind(size(G),2*ones(size(edges_00_10)),edges_00_10)) = -1000;
            
            % For all edges, where sample(node_i) = 1 and node_i observed 
            % variable, 
            % put G(0*,(node_i,*)) = -1000 and G(*0,(*,node_i)) = -1000
            [~,ind] = ismember(edge_list(:,1),nodes_0);
            edges_10_11 = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),nodes_0);
            edges_01_11 = find(ind ~= 0);
            
            G(sub2ind(size(G),2*ones(size(edges_10_11)),edges_10_11)) = -1000;
            G(sub2ind(size(G),4*ones(size(edges_10_11)),edges_10_11)) = -1000;
            G(sub2ind(size(G),3*ones(size(edges_01_11)),edges_01_11)) = -1000;
            G(sub2ind(size(G),4*ones(size(edges_01_11)),edges_01_11)) = -1000;
                         
            trueLogZ = solveConditionalDAI(F,G,edge_list);
            sample_conditional_likelihood = compute_sample_likelihood_overcomplete(F,G,trueLogZ,sample,modified_structure);
        end
        
        % This function implements slicing to get approximate inference
        function [sample_conditional_likelihood] = compute_conditional_likelihood_for_sample_approx(self,sample,observed_variables)
            % modify parameters, i.e. overcomplete node potentials 
            F = self.theta.F;
            G = self.theta.G;
            
            % For all nodes, where sample(node) == 1, put F(1,node) = small
            % Find all such nodes in the sample
            nodes_1 = intersect(find(sample == 1),observed_variables);
            F_indices = 1*ones(1,length(nodes_1));
            F(sub2ind(size(F),F_indices,nodes_1)) = -1000;
            
            % For all nodes, where sample(node) == 0, put F(2,node) = small
            % Find all such nodes in the sample
            nodes_0 = intersect(find(sample == 0),observed_variables);
            F_indices = 2*ones(1,length(nodes_0));
            F(sub2ind(size(F),F_indices,nodes_0)) = -1000;
            
            % Get overcomplete parametrization for the sample
            overcomplete_struct = samples_to_overcomplete(sample, self.structure);
            edge_list = overcomplete_struct.edges{1}';
            
            % Remove all the edges where both nodes are observed
            [~,ind] = ismember(edge_list(:,1),observed_variables);
            observed_edges = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),observed_variables);
            observed_edges = intersect(observed_edges,find(ind ~= 0));
            edge_list(observed_edges,:) = [];
            G(:,observed_edges) = [];
            
            % Recompute overcomplete struct
            % change structure
            modified_structure = self.structure;
            x = repmat(observed_variables',length(observed_variables),1);
            
            y = repmat(observed_variables',1,length(observed_variables))';
            y = y(:);
            
            modified_structure(sub2ind(size(modified_structure),x,y)) = 0;
            overcomplete_struct = samples_to_overcomplete(sample, modified_structure);
            
            % For all edges, where sample(node_i) = 1 and node_i observed 
            % variable, 
            % put G(1*,(node_i,*)) = -1000 and G(*1,(*,node_i)) = -1000
            [~,ind] = ismember(edge_list(:,1),nodes_1);
            edges_00_01 = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),nodes_1);
            edges_00_10 = find(ind ~= 0);
            
            G(sub2ind(size(G),ones(size(edges_00_01)),edges_00_01)) = -1000;
            G(sub2ind(size(G),2*ones(size(edges_00_01)),edges_00_01)) = -1000;
            G(sub2ind(size(G),ones(size(edges_00_10)),edges_00_10)) = -1000;
            G(sub2ind(size(G),3*ones(size(edges_00_10)),edges_00_10)) = -1000;
            
            % For all edges, where sample(node_i) = 0 and node_i observed 
            % variable, 
            % put G(0*,(node_i,*)) = -1000 and G(*0,(*,node_i)) = -1000
            [~,ind] = ismember(edge_list(:,1),nodes_0);
            edges_10_11 = find(ind ~= 0);
            [~,ind] = ismember(edge_list(:,2),nodes_0);
            edges_01_11 = find(ind ~= 0);
            
            G(sub2ind(size(G),3*ones(size(edges_10_11)),edges_10_11)) = -1000;
            G(sub2ind(size(G),4*ones(size(edges_10_11)),edges_10_11)) = -1000;
            G(sub2ind(size(G),2*ones(size(edges_01_11)),edges_01_11)) = -1000;
            G(sub2ind(size(G),4*ones(size(edges_01_11)),edges_01_11)) = -1000;
                         
            
            % Approximate Test Likelihood
            [~, YNtrueFlat] = max(overcomplete_struct.YN, [], 2); 
            tobj  = IsingPredict(F, G, overcomplete_struct.Ut, overcomplete_struct.Vt,...
                overcomplete_struct.Ns(1), overcomplete_struct.edges{1}, 'YNtrueFlat', YNtrueFlat, self.approx_predict_opts{:});
            tprob = BatchFW(tobj, 'MaxIter', self.max_prediction_iter, ...
                'printInterval', 100, 'linesearch', false, self.approx_predict_opts{:});
            tprob.run();
            sample_conditional_likelihood = compute_sample_likelihood_overcomplete(F,G,-1*tprob.obj.fval(),sample,modified_structure);
        end
          
    end    
end

